{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4acd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from torch_optimizer import Ranger\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from multiprocessing import cpu_count\n",
    "from os import environ\n",
    "from platform import system\n",
    "from utilities.utils import *\n",
    "\n",
    "environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "pl.seed_everything(seed=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675f0f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2.5e-2\n",
    "BATCH_SIZE = 128\n",
    "WEIGHT_DECAY = 1e-2\n",
    "EPOCHS = 25\n",
    "MAX_LEN = None\n",
    "CELL_TYPE = \"gru\"\n",
    "N_JOBS = cpu_count() if system() != \"Windows\" else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8793db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEQ(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 cell_dim=128,\n",
    "                 embed_dim=128,\n",
    "                 num_layers=3,\n",
    "                 dropout=0.1,\n",
    "                 cell_type=CELL_TYPE,\n",
    "                 bidirectional=True,\n",
    "                 use_scheduler=True,\n",
    "                 train_dataset=None,\n",
    "                 val_dataset=None,\n",
    "                 test_dataset=None):\n",
    "\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=input_dim,\n",
    "                                      embedding_dim=embed_dim,\n",
    "                                      padding_idx=0)\n",
    "        \n",
    "\n",
    "        c = (2 if bidirectional else 1)\n",
    "        k = (2 if cell_type == \"lstm\" else 1)\n",
    "        if cell_type == \"lstm\":\n",
    "            self.cell = nn.LSTM(input_size=embed_dim, \n",
    "                                hidden_size=cell_dim, \n",
    "                                dropout=dropout,\n",
    "                                batch_first=True,\n",
    "                                num_layers=num_layers, \n",
    "                                bidirectional=bidirectional)\n",
    "        else:\n",
    "            self.cell = nn.GRU(input_size=embed_dim, \n",
    "                               hidden_size=cell_dim, \n",
    "                               dropout=dropout,\n",
    "                               batch_first=True,\n",
    "                               num_layers=num_layers, \n",
    "                               bidirectional=bidirectional)\n",
    "            \n",
    "        fc_dim = k * c * num_layers * cell_dim\n",
    "        self.fc = nn.Linear(fc_dim, 2)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.use_scheduler = use_scheduler\n",
    "        self.loss_fn = nn.NLLLoss()\n",
    "        self.cell_type = cell_type\n",
    "        self.adaptive_avg_pool = nn.AdaptiveAvgPool1d(cell_dim)\n",
    "        ## Datasets ##\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        ## metrics ##\n",
    "        self.train_f1score = torchmetrics.F1Score()\n",
    "        self.val_f1score = torchmetrics.F1Score()\n",
    "        self.test_f1score = torchmetrics.F1Score()\n",
    "        ## Hyperparameters ##\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.learning_rate = LEARNING_RATE\n",
    "        self.weight_decay = WEIGHT_DECAY\n",
    "        if self.use_scheduler:\n",
    "            self.total_steps = len(train_dataset) // self.batch_size\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, \n",
    "                          batch_size=self.batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=N_JOBS,\n",
    "                          drop_last=False)\n",
    "\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, \n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=N_JOBS,\n",
    "                          drop_last=False)\n",
    "\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, \n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=N_JOBS,\n",
    "                          drop_last=False)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, \n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=N_JOBS,\n",
    "                          drop_last=False)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        _, out = self.cell(self.embedding(input_ids))\n",
    "        if self.cell_type == \"lstm\": torch.cat(out, dim=-1)\n",
    "        out = torch.reshape(out, (out.size(1), -1))\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        out = F.log_softmax(out, dim=-1)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def _shared_evaluation_step(self, batch, batch_idx):\n",
    "        ids, lbls = batch\n",
    "        out = self(ids)\n",
    "        loss = self.loss_fn(out, lbls)\n",
    "        preds = torch.argmax(out, dim=-1)\n",
    "        return loss, preds\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, preds = self._shared_evaluation_step(batch, batch_idx)\n",
    "        self.train_f1score(preds, batch[-1])\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_f1score\", self.train_f1score, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, preds = self._shared_evaluation_step(batch, batch_idx)\n",
    "        self.val_f1score(preds, batch[-1])\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_f1score\", self.val_f1score, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, preds = self._shared_evaluation_step(batch, batch_idx)\n",
    "        self.test_f1score(preds, batch[-1])\n",
    "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_f1score\", self.test_f1score, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        ids, _ = batch\n",
    "        return torch.argmax(self(ids), dim=-1)\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Ranger(self.parameters(),\n",
    "                           lr=self.learning_rate,\n",
    "                           weight_decay=self.weight_decay)\n",
    "\n",
    "        if self.use_scheduler:\n",
    "            scheduler = get_cosine_schedule_with_warmup(optimizer=optimizer,\n",
    "                                                        num_warmup_steps=1,\n",
    "                                                        num_training_steps=self.total_steps)\n",
    "            lr_scheduler = {\n",
    "                'scheduler': scheduler, \n",
    "                'interval': 'epoch', \n",
    "                'frequency': 1\n",
    "            }\n",
    "            return [optimizer], [lr_scheduler]\n",
    "        else:\n",
    "            return [optimizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f430d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/full_vocab_sug_tree_bank_tokenier.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    vocab = [s.strip() for s in f.readlines()]\n",
    "    VOCAB2IDX = {v:k for (k, v) in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc608a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_ids, labels = get_encoded_input(\"../data/App_Training.csv\", \n",
    "                                      maxlen=MAX_LEN, \n",
    "                                      vocab2idx=VOCAB2IDX)\n",
    "\n",
    "dataset = TensorDataset(torch.LongTensor(input_ids),\n",
    "                        torch.LongTensor(labels))\n",
    "\n",
    "L = len(labels)\n",
    "\n",
    "train_sz, val_sz = L-int(0.1*L), int(0.1*L)\n",
    "train_dataset, val_dataset = random_split(dataset, (train_sz, val_sz))                                                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b368da",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, labels = get_encoded_input(\"../data/App_Test_Labeled.csv\", \n",
    "                                      maxlen=MAX_LEN, \n",
    "                                      vocab2idx=VOCAB2IDX)\n",
    "\n",
    "test_dataset = TensorDataset(torch.LongTensor(input_ids),\n",
    "                             torch.LongTensor(labels))                                                                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328834d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SEQ(input_dim=len(VOCAB2IDX),\n",
    "            cell_type=CELL_TYPE,\n",
    "            train_dataset=train_dataset,\n",
    "            val_dataset=val_dataset,\n",
    "            test_dataset=test_dataset,\n",
    "            use_scheduler=True)\n",
    "\n",
    "earlystopping_callback = EarlyStopping(monitor=\"val_f1score\", \n",
    "                                       min_delta=1e-4, \n",
    "                                       patience=10, \n",
    "                                       mode=\"max\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=\"./saved_weights\",\n",
    "                                      filename=f\"seq-{CELL_TYPE}-sug\",\n",
    "                                      save_top_k=1, \n",
    "                                      mode=\"max\",\n",
    "                                      monitor=\"val_f1score\",\n",
    "                                      save_weights_only=True)\n",
    "\n",
    "trainer = pl.Trainer(accelerator=\"gpu\",\n",
    "                     max_epochs=EPOCHS,\n",
    "                     precision=16,\n",
    "                     log_every_n_steps=1,\n",
    "                     callbacks=[earlystopping_callback, \n",
    "                                checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fe7662",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c04fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f\"./saved_weights/seq-{CELL_TYPE}-sug.ckpt\")[\"state_dict\"])\n",
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbbd3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "7a2000831a5050d8503f24d6733c4641a8734e9c81b6dced7c2deb928c6c3201"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
