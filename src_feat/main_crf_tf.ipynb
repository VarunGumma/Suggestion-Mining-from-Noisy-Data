{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4acd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import Mean\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow_addons.layers import CRF\n",
    "from tensorflow_addons.text import crf_log_likelihood\n",
    "from tensorflow_addons.metrics import F1Score\n",
    "from tensorflow_addons.optimizers import Lookahead, SGDW\n",
    "from metrics import f1score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas import read_csv\n",
    "from ast import literal_eval\n",
    "from itertools import chain\n",
    "from utils import post_pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c37973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1.5\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 128\n",
    "MAX_LEN = 128\n",
    "WEIGHT_DECAY = 1e-3\n",
    "\n",
    "tag2idx = {'B': 0, 'I': 1, 'O': 2, 'E': 3, 'S': 4, '<': 5, \">\":6, \"$\": 7}\n",
    "pos2idx = {\"NOUN\": 0, \"PROPN\": 1, \"VERB\": 2, \"ADJ\": 3, \"OTHER\": 4, \"<START>\": 5, \"<END>\": 6, \"<PAD>\": 7}\n",
    "\n",
    "convert_to_one_hot = lambda values: np.eye(len(tag2idx), dtype=int)[values].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d16cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_crf_model(tag_size=len(tag2idx)):\n",
    "    x = Input(shape=(MAX_LEN, tag_size), dtype=tf.int32)\n",
    "    decoded_sequence, potentials, sequence_length, kernel = CRF(tag_size)(x)\n",
    "    return Model(inputs=x, outputs=[decoded_sequence, potentials, sequence_length, kernel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9d41bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crf_loss_func(potentials, y, sequence_lengths, kernel):\n",
    "    crf_likelihood, _ = crf_log_likelihood(potentials, y, sequence_lengths, kernel)\n",
    "    crf_loss = tf.reduce_mean(-crf_likelihood)\n",
    "    return crf_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87680116",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_csv(\"../data/train_290818.txt\", \n",
    "                sep=\" \",\n",
    "                header=None, \n",
    "                encoding=\"utf-8\").values.tolist()\n",
    "\n",
    "text = [literal_eval(words) for (words, _, _) in data]\n",
    "text = [[token.pos_ for token in nlp(' '.join(s))] for s in text]\n",
    "text = [[p if (p == \"NOUN\" or p == \"PROPN\" or p == \"VERB\" or p == \"ADJ\") else \"OTHER\" for p in sent] for sent in text]\n",
    "text = post_pad_sequences(text, maxlen=MAX_LEN, return_masks=True)\n",
    "encoded_input = np.array([to_categorical([pos2idx[p] for p in sent], num_classes=len(tag2idx)) for sent in text[\"seq\"]])\n",
    "\n",
    "labels = [[l.split('-')[0] for l in literal_eval(labels)] for (_, labels, _) in data]\n",
    "labels = post_pad_sequences(labels, maxlen=MAX_LEN, return_masks=False, start='<', end='>', pad='$')\n",
    "extended_labels = [[tag2idx[l] for l in lbls] for lbls in labels[\"seq\"]]\n",
    "\n",
    "train_dataset = Dataset.from_tensor_slices((encoded_input, text[\"mask\"], extended_labels)).batch(batch_size=BATCH_SIZE)                                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b195cebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_csv(\"../data/test_290818.txt\", \n",
    "                sep=\" \",\n",
    "                header=None, \n",
    "                encoding=\"utf-8\").values.tolist()\n",
    "\n",
    "text = [literal_eval(words) for (words, _, _) in data]\n",
    "text = [[token.pos_ for token in nlp(' '.join(s))] for s in text]\n",
    "text = [[p if (p == \"NOUN\" or p == \"PROPN\" or p == \"VERB\" or p == \"ADJ\") else \"OTHER\" for p in sent] for sent in text]\n",
    "text = post_pad_sequences(text, maxlen=MAX_LEN, return_masks=True)\n",
    "encoded_input = np.array([to_categorical([pos2idx[p] for p in sent], num_classes=len(tag2idx)) for sent in text[\"seq\"]])\n",
    "\n",
    "labels = [[l.split('-')[0] for l in literal_eval(labels)] for (_, labels, _) in data]\n",
    "labels = post_pad_sequences(labels, maxlen=MAX_LEN, start='<', end='>', pad='$', return_masks=False)\n",
    "extended_labels = [[tag2idx[l] for l in lbls] for lbls in labels[\"seq\"]]\n",
    "\n",
    "test_dataset = Dataset.from_tensor_slices((encoded_input, text[\"mask\"], extended_labels)).batch(batch_size=BATCH_SIZE)                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39b7091",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_crf_model()\n",
    "\n",
    "optimizer = Lookahead(SGDW(learning_rate=LEARNING_RATE, momentum=0.9, nesterov=True, weight_decay=weight_decay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0284961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x, mask, y):\n",
    "    losses, f1scores = [], []\n",
    "    with tf.GradientTape() as tape:\n",
    "        yp, potentials, sequence_length, kernel = model(x, mask=mask, training=True)        \n",
    "        yp = tf.reverse(yp, axis=[-1])\n",
    "        loss = crf_loss_func(potentials, y, sequence_length, kernel)\n",
    "        f1scores.append(f1score(y.numpy(), yp.numpy()))\n",
    "        losses.append(loss)            \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss, f1scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e307c8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_step(x, mask, y):\n",
    "    losses, f1scores = [], []\n",
    "    yp, potentials, sequence_length, kernel = model(x, mask=mask, training=False)        \n",
    "    yp = tf.reverse(yp, axis=[-1])\n",
    "    loss = crf_loss_func(potentials, y, sequence_length, kernel)\n",
    "    f1scores.append(f1score(y.numpy(), yp.numpy()))\n",
    "    losses.append(loss)            \n",
    "    return loss, f1scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1130f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, EPOCHS+1):\n",
    "    losses, f1scores = [], []\n",
    "    for batch in train_dataset:\n",
    "        loss, f1 = train_step(*batch)\n",
    "        f1scores.append(f1)\n",
    "        losses.append(loss)\n",
    "    print(f\"loss: {np.mean(loss):.5f}, f1score: {np.mean(f1scores):.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
