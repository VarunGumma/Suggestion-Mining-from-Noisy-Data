{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba4acd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchcrf import CRF\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW, RMSprop\n",
    "from torch_optimizer import SGDW, Lookahead\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import get_encoded_input\n",
    "from metrics import f1score\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from multiprocessing import cpu_count\n",
    "from platform import system\n",
    "from os import environ\n",
    "from math import sqrt\n",
    "\n",
    "environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "pl.seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "675f0f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-8\n",
    "BATCH_SIZE = 64\n",
    "WEIGHT_DECAY = 5e-7\n",
    "EPOCHS = 25\n",
    "MAX_LEN = 128\n",
    "N_JOBS = cpu_count() if system() != \"Windows\" else 0\n",
    "\n",
    "tag2idx = {'B': 0, 'I': 1, 'O': 2, 'E': 3, 'S': 4, 'X': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8793db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM4NER(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 bidirectional=False, \n",
    "                 num_lstm_layers=1,\n",
    "                 embed_dim=128, \n",
    "                 dropout=0.1, \n",
    "                 lstm_dim=128,\n",
    "                 num_tags=len(tag2idx),\n",
    "                 train_dataset=None,\n",
    "                 val_dataset=None,\n",
    "                 test_dataset=None):\n",
    "\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=input_dim,\n",
    "                                      embedding_dim=embed_dim,\n",
    "                                      padding_idx=0)\n",
    "        \n",
    "\n",
    "        c = (2 if bidirectional else 1)\n",
    "        self.lstm = nn.LSTM(input_size=embed_dim, \n",
    "                            hidden_size=lstm_dim, \n",
    "                            dropout=dropout,\n",
    "                            num_layers=num_lstm_layers, \n",
    "                            bidirectional=bidirectional)\n",
    "\n",
    "        \n",
    "        self.fc = nn.Linear(lstm_dim*c, num_tags)\n",
    "        self.crf = CRF(num_tags=num_tags, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(p=dropout)\n",
    "        self.dropout2 = nn.Dropout(p=dropout)\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=lstm_dim*c,\n",
    "                                         num_heads=1,\n",
    "                                         dropout=dropout,\n",
    "                                         batch_first=True)\n",
    "        ## Hyperparameters ##\n",
    "        self.learning_rate = LEARNING_RATE\n",
    "        self.weight_decay = WEIGHT_DECAY\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        ## Datasets ##\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, \n",
    "                          batch_size=self.batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=N_JOBS,\n",
    "                          drop_last=False)\n",
    "\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, \n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=N_JOBS,\n",
    "                          drop_last=False)\n",
    "\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, \n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=N_JOBS,\n",
    "                          drop_last=False)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, masks):\n",
    "        out = self.embedding(input_ids)\n",
    "        out, _ = self.lstm(out)\n",
    "        out = F.mish(out)\n",
    "        out = self.dropout1(out)\n",
    "        out, _ = self.mha(query=out, \n",
    "                          key=out, \n",
    "                          value=out, \n",
    "                          key_padding_mask=~masks,\n",
    "                          need_weights=False)\n",
    "        out = F.mish(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "    def _shared_evaluation_step(self, batch, batch_idx):\n",
    "        ids, masks, lbls = batch\n",
    "        emissions = self(ids, masks)\n",
    "        loss = -self.crf(emissions, lbls, mask=masks)\n",
    "        pred = self.crf.decode(emissions, mask=masks)\n",
    "        r, p, f1 = f1score(lbls, pred)\n",
    "        return loss, r, p, f1\n",
    "     \n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, r, p, f1 = self._shared_evaluation_step(batch, batch_idx)\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_recall\", r, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_precision\", p, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_f1score\", f1, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, r, p, f1 = self._shared_evaluation_step(batch, batch_idx)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_recall\", r, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_precision\", p, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_f1score\", f1, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, r, p, f1 = self._shared_evaluation_step(batch, batch_idx)\n",
    "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_recall\", r, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_precision\", p, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_f1score\", f1, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        ids, masks, _ = batch\n",
    "        return self.crf.decode(self(ids), mask=masks)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):  \n",
    "        return SGDW(self.parameters(),\n",
    "                    lr=self.learning_rate,\n",
    "                    momentum=0.9,\n",
    "                    nesterov=True,\n",
    "                    weight_decay=self.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8733e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/full_vocab_290818_tree_bank_tokenier.txt\", \"r\") as f:\n",
    "    vocab = [s.strip() for s in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5875c015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     2187.000000\n",
      "mean        18.227252\n",
      "std         14.683643\n",
      "min          4.000000\n",
      "25%         10.000000\n",
      "50%         14.000000\n",
      "75%         21.000000\n",
      "90%         32.000000\n",
      "95%         40.700000\n",
      "99%         82.420000\n",
      "99.9%      152.512000\n",
      "99.99%     200.758400\n",
      "max        213.000000\n",
      "Name: seq_len, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "encoded_input, attn_masks, extended_labels = get_encoded_input(\"../../data/train_290818.txt\", \n",
    "                                                               tag2idx=tag2idx,\n",
    "                                                               vocab=vocab,\n",
    "                                                               visualize=True,\n",
    "                                                               max_len=MAX_LEN)\n",
    "\n",
    "input_ids_train, input_ids_val, attn_masks_train, attn_masks_val, extended_labels_train, extended_labels_val = train_test_split(encoded_input,\n",
    "                                                                                                                                attn_masks,\n",
    "                                                                                                                                extended_labels, \n",
    "                                                                                                                                test_size=0.1, \n",
    "                                                                                                                                shuffle=True) \n",
    "\n",
    "input_ids_train = torch.LongTensor(input_ids_train)\n",
    "attn_masks_train = torch.BoolTensor(attn_masks_train)\n",
    "extended_labels_train = torch.LongTensor(extended_labels_train)\n",
    "\n",
    "input_ids_val = torch.LongTensor(input_ids_val)\n",
    "attn_masks_val = torch.BoolTensor(attn_masks_val)\n",
    "extended_labels_val = torch.LongTensor(extended_labels_val)\n",
    "\n",
    "train_dataset = TensorDataset(input_ids_train, attn_masks_train, extended_labels_train)\n",
    "val_dataset = TensorDataset(input_ids_val, attn_masks_val, extended_labels_val)                                                                                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8bc2e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     547.00000\n",
      "mean       17.08958\n",
      "std        12.16015\n",
      "min         4.00000\n",
      "25%         9.00000\n",
      "50%        13.00000\n",
      "75%        20.50000\n",
      "90%        30.40000\n",
      "95%        39.00000\n",
      "99%        65.70000\n",
      "99.9%      95.08600\n",
      "99.99%     99.50860\n",
      "max       100.00000\n",
      "Name: seq_len, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "encoded_input, attn_masks, extended_labels = get_encoded_input(\"../../data/test_290818.txt\", \n",
    "                                                               tag2idx=tag2idx,\n",
    "                                                               vocab=vocab,\n",
    "                                                               visualize=True,\n",
    "                                                               max_len=MAX_LEN)\n",
    "\n",
    "input_ids_test = torch.LongTensor(encoded_input)\n",
    "attn_masks_test = torch.BoolTensor(attn_masks)\n",
    "extended_labels_test = torch.LongTensor(extended_labels) \n",
    "\n",
    "test_dataset = TensorDataset(input_ids_test, attn_masks_test, extended_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "328834d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/varun/.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model = LSTM4NER(input_dim=len(vocab),\n",
    "                 bidirectional=True,\n",
    "                 train_dataset=train_dataset,\n",
    "                 val_dataset=val_dataset,\n",
    "                 test_dataset=test_dataset)\n",
    "\n",
    "earlystopping_callback = EarlyStopping(monitor=\"val_f1score\", \n",
    "                                       min_delta=1e-4, \n",
    "                                       patience=5, \n",
    "                                       mode=\"max\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=\"./\",\n",
    "                                      filename=f\"lstm-ner-val-f1score\",\n",
    "                                      save_top_k=1, \n",
    "                                      mode=\"max\",\n",
    "                                      monitor=\"val_f1score\",\n",
    "                                      save_weights_only=True)\n",
    "\n",
    "trainer = pl.Trainer(accelerator=\"gpu\",\n",
    "                     max_epochs=EPOCHS,\n",
    "                     precision=16,\n",
    "                     log_every_n_steps=1,\n",
    "                     callbacks=[earlystopping_callback, checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fe7662",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/varun/.local/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:611: UserWarning: Checkpoint directory /run/media/varun/New Volume/NUS_Project/src_feat/old files exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | embedding | Embedding          | 612 K \n",
      "1 | lstm      | LSTM               | 264 K \n",
      "2 | fc        | Linear             | 1.5 K \n",
      "3 | crf       | CRF                | 48    \n",
      "4 | dropout1  | Dropout            | 0     \n",
      "5 | dropout2  | Dropout            | 0     \n",
      "6 | mha       | MultiheadAttention | 263 K \n",
      "-------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "2.283     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "385ba29fd9584290a1621ea636985f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/varun/.local/lib/python3.10/site-packages/torch_optimizer/sgdw.py:112: UserWarning: This overload of add is deprecated:\n",
      "\tadd(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1174.)\n",
      "  d_p = d_p.add(momentum, buf)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c04fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f\"./lstm-ner-val-f1score.ckpt\")[\"state_dict\"])\n",
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e95a09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "7a2000831a5050d8503f24d6733c4641a8734e9c81b6dced7c2deb928c6c3201"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
