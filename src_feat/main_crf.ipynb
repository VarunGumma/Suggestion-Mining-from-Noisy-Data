{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4acd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchcrf import CRF\n",
    "import torch\n",
    "import spacy\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from metrics import f1score as f1score_featurewise\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from multiprocessing import cpu_count\n",
    "from platform import system\n",
    "from os import environ\n",
    "from sklearn.metrics import f1_score as f1score_elementwise\n",
    "from pandas import read_csv\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "\n",
    "environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "pl.seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7423ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_one_hot = lambda values: np.eye(len(tag2idx), dtype=int)[values].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe0867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_pad_sequences(sequences, max_len=None, pad_value=0):\n",
    "    longest = max([len(x) for x in sequences])\n",
    "    if max_len is not None and max_len > longest:\n",
    "        print(\"Unnecessary extra padding detected!\\nSequences will be only be padded to the longest length\")\n",
    "    sequences = [(x + [pad_value]*(longest-len(x))) for x in sequences]\n",
    "    return sequences if max_len is None or max_len >= longest else [x[:max_len] for x in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675f0f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2e-3\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 25\n",
    "N_JOBS = cpu_count() if system() != \"Windows\" else 0\n",
    "\n",
    "tag2idx = {'B': 0, 'I': 1, 'O': 2, 'E': 3, 'S': 4, 'X': 5}\n",
    "pos2idx = {\"NOUN\": 0, \"VERB\": 1, \"ADJ\": 2, \"ADV\": 3, \"PRON\": 4, \"OTHER\": 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8793db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRF4NER(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 eval_metric=\"feature-wise\",\n",
    "                 num_tags=len(tag2idx),\n",
    "                 train_dataset=None,\n",
    "                 val_dataset=None,\n",
    "                 test_dataset=None):\n",
    "\n",
    "        super().__init__()\n",
    "        self.crf = CRF(num_tags=num_tags, batch_first=True)\n",
    "        self.eval_metric = f1score_featurewise if eval_metric == \"feature-wise\" else f1score_elementwise\n",
    "        ## Hyperparameters ##\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.learning_rate = LEARNING_RATE\n",
    "        ## Datasets ##\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, \n",
    "                          batch_size=self.batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=N_JOBS,\n",
    "                          drop_last=False)\n",
    "\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, \n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=N_JOBS,\n",
    "                          drop_last=False)\n",
    "\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, \n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=N_JOBS,\n",
    "                          drop_last=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def _shared_evaluation_step(self, batch, batch_idx):\n",
    "        ids, masks, lbls = batch\n",
    "        loss = -self.crf(ids, lbls, mask=masks)\n",
    "        pred = self.crf.decode(ids, mask=masks)\n",
    "        r, p, f1 = self.eval_metric(lbls.tolist(), pred)\n",
    "        return loss, r, p, f1\n",
    "    \n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, r, p, f1 = self._shared_evaluation_step(batch, batch_idx)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_recall\", r, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_precision\", p, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_f1score\", f1, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, r, p, f1 = self._shared_evaluation_step(batch, batch_idx)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_recall\", r, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_precision\", p, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_f1score\", f1, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, r, p, f1 = self._shared_evaluation_step(batch, batch_idx)\n",
    "        self.log(\"test_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_recall\", r, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_precision\", p, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_f1score\", f1, on_epoch=True, prog_bar=True)\n",
    "\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        ids, masks = batch \n",
    "        return self.crf.decode(ids, mask=masks)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return SGD(self.parameters(),\n",
    "                   lr=self.learning_rate,\n",
    "                   momentum=0.9,\n",
    "                   nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87680116",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_csv(\"../../data/train_290818.txt\", \n",
    "                sep=\" \",\n",
    "                header=None, \n",
    "                encoding=\"utf-8\").values.tolist()\n",
    "\n",
    "text = [literal_eval(words) for (words, _, _) in data]\n",
    "labels = [[l.split('-')[0] for l in literal_eval(labels)] for (_, labels, _) in data]\n",
    "pos = [[token.pos_ for token in nlp(' '.join(s))] for s in text]\n",
    "\n",
    "\n",
    "pos_new = []\n",
    "for sent in pos:\n",
    "    temp = []\n",
    "    for p in sent:\n",
    "        if p == \"NOUN\" or p == \"PROPN\":\n",
    "            temp.append(\"NOUN\")\n",
    "        elif p == \"ADV\" or p == \"VERB\" or p == \"ADJ\" or p == \"PRON\":\n",
    "            temp.append(p)\n",
    "        else:\n",
    "            temp.append(\"OTHER\")\n",
    "    pos_new.append(temp)\n",
    "\n",
    "\n",
    "primitive_lens = [len(x) for x in pos_new]\n",
    "pos_new = post_pad_sequences(pos_new, pad_value=\"OTHER\")\n",
    "fl = len(pos_new[0])\n",
    "\n",
    "encoded_input = [convert_to_one_hot([pos2idx[p] for p in sent]) for sent in pos_new]\n",
    "attn_masks = [([1]*pl + [0]*(fl - pl)) for pl in primitive_lens]\n",
    "\n",
    "labels = [(l + ['X']*(fl - len(l))) for l in labels]\n",
    "extended_labels = [[tag2idx[l] for l in lbls] for lbls in labels]\n",
    "\n",
    "\n",
    "input_ids_train, input_ids_val, attn_masks_train, attn_masks_val, extended_labels_train, extended_labels_val = train_test_split(encoded_input,\n",
    "                                                                                                                                attn_masks,\n",
    "                                                                                                                                extended_labels, \n",
    "                                                                                                                                test_size=0.1, \n",
    "                                                                                                                                shuffle=True) \n",
    "\n",
    "input_ids_train = torch.LongTensor(input_ids_train)\n",
    "attn_masks_train = torch.BoolTensor(attn_masks_train)\n",
    "extended_labels_train = torch.LongTensor(extended_labels_train)\n",
    "\n",
    "input_ids_val = torch.LongTensor(input_ids_val)\n",
    "attn_masks_val = torch.BoolTensor(attn_masks_val)\n",
    "extended_labels_val = torch.LongTensor(extended_labels_val)\n",
    "\n",
    "train_dataset = TensorDataset(input_ids_train, attn_masks_train, extended_labels_train)\n",
    "val_dataset = TensorDataset(input_ids_val, attn_masks_val, extended_labels_val)                                                                                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b195cebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_csv(\"../../data/test_290818.txt\", \n",
    "                sep=\" \",\n",
    "                header=None, \n",
    "                encoding=\"utf-8\").values.tolist()\n",
    "\n",
    "text = [literal_eval(words) for (words, _, _) in data]\n",
    "labels = [[l.split('-')[0] for l in literal_eval(labels)] for (_, labels, _) in data]\n",
    "pos = [[token.pos_ for token in nlp(' '.join(s))] for s in text]\n",
    "\n",
    "\n",
    "pos_new = []\n",
    "for sent in pos:\n",
    "    temp = []\n",
    "    for p in sent:\n",
    "        if p == \"NOUN\" or p == \"PROPN\":\n",
    "            temp.append(\"NOUN\")\n",
    "        elif p == \"ADV\" or p == \"VERB\" or p == \"ADJ\" or p == \"PRON\":\n",
    "            temp.append(p)\n",
    "        else:\n",
    "            temp.append(\"OTHER\")\n",
    "    pos_new.append(temp)\n",
    "\n",
    "\n",
    "primitive_lens = [len(x) for x in pos_new]\n",
    "pos_new = post_pad_sequences(pos_new, pad_value=\"OTHER\")\n",
    "fl = len(pos_new[0])\n",
    "\n",
    "encoded_input = np.array([convert_to_one_hot([pos2idx[p] for p in sent]) for sent in pos_new])\n",
    "attn_masks = [([1]*pl + [0]*(fl - pl)) for pl in primitive_lens]\n",
    "labels = [(l + ['X']*(fl - len(l))) for l in labels]\n",
    "extended_labels = [[tag2idx[l] for l in lbls] for lbls in labels]\n",
    "\n",
    "input_ids_test = torch.LongTensor(encoded_input)\n",
    "attn_masks_test = torch.BoolTensor(attn_masks)\n",
    "extended_labels_test = torch.LongTensor(extended_labels)\n",
    "\n",
    "test_dataset = TensorDataset(input_ids_test, attn_masks_test, extended_labels_test)                                                                                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328834d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CRF4NER(train_dataset=train_dataset,\n",
    "                val_dataset=val_dataset,\n",
    "                test_dataset=test_dataset)\n",
    "\n",
    "earlystopping_callback = EarlyStopping(monitor=\"val_f1score\", \n",
    "                                       min_delta=1e-4, \n",
    "                                       patience=5, \n",
    "                                       mode=\"max\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=\"./\",\n",
    "                                      filename=\"crf-ner-val-f1score\",\n",
    "                                      save_top_k=1, \n",
    "                                      mode=\"max\",\n",
    "                                      monitor=\"val_f1score\",\n",
    "                                      save_weights_only=True)\n",
    "\n",
    "trainer = pl.Trainer(accelerator=\"gpu\",\n",
    "                     max_epochs=EPOCHS,\n",
    "                     precision=16,\n",
    "                     log_every_n_steps=1,\n",
    "                     callbacks=[earlystopping_callback, checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fe7662",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c04fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f\"./crf-ner-val-f1score.ckpt\")[\"state_dict\"])\n",
    "trainer.test(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "7a2000831a5050d8503f24d6733c4641a8734e9c81b6dced7c2deb928c6c3201"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
